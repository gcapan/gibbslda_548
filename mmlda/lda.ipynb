{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as spar\n",
    "import scipy.special as spec\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = 1000 # nr words in vocabulary\n",
    "M = 10 # nr documents\n",
    "K = 5 # nr of topics\n",
    "alpha = .1 # dirichlet hyperparameter\n",
    "\n",
    "X = np.random.binomial(1,.1, size=M*V).reshape(M,V)\n",
    "X = spar.csr_matrix(X, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For even a reasonable setup like 10K vocabulary, 5K documents and 20 topics, the size of the tensor indexed by\n",
    "# <document, word, topic> simply explodes to 7.5G. This is why we can't explicitly keep all of $\\phi$ in the memory.\n",
    "# Instead, we iterate over the documents one by one, and accumulate the phi parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_terms = X.sum(axis=1) \n",
    "nr_terms = np.array(nr_terms).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new doc\n",
      "(14.14213562373085, 53.889238257744935)\n",
      "(0.0, 393.54796403996306, 14.14213562373085, 447.43720229770798)\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 48.075461516245475)\n",
      "(0.0, 399.36174078146252, 14.14213562373085, 447.43720229770798)\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n",
      "new doc\n",
      "(14.14213562373085, 447.43720229770798)\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "beta = np.zeros((K, V)) + 1./V\n",
    "\n",
    "# variational and temp variables\n",
    "gamma = np.zeros((K, M)) + alpha + (nr_terms/float(K)) # mth document, i th topic\n",
    "beta_acc = np.ones((K, V))\n",
    "\n",
    "for epoch in range(5):\n",
    "    # E-step\n",
    "    for m in range(2): # iterate over all documents\n",
    "        print \"new doc\"\n",
    "        phi = np.zeros((K, V), dtype=float) + 1./K\n",
    "\n",
    "        ixw = (X[m, :] > 0).toarray().squeeze() # an index to words which have appeared in the document\n",
    "        gammad = gamma[:, m] # slice for the document only once\n",
    "\n",
    "        for ctr in range(int(100)): \n",
    "            # store the previous values\n",
    "            phi_prev = phi.copy()\n",
    "            gammad_prev = gammad.copy()\n",
    "            \n",
    "            print (np.linalg.norm(phi_prev), np.linalg.norm(gammad_prev))\n",
    "\n",
    "            # update phi\n",
    "            # WARN: exp digamma underflows < 1e-3! \n",
    "            # TODO: digamma update is wrong! -spec.digamma(np.sum(gammad))\n",
    "            phi[:, ixw] = ((beta[:, ixw]).T * np.exp(spec.digamma(gammad))).T \n",
    "            phi = phi / np.sum(phi, 0) # normalize phi columns\n",
    "\n",
    "            # update gamma\n",
    "            gammad = alpha + np.sum(phi, axis=1)\n",
    "\n",
    "            # check for convergence\n",
    "            dphinorm = np.linalg.norm(phi - phi_prev, \"fro\") #/ float(K * V)\n",
    "            dgammadnorm = np.linalg.norm(gammad - gammad_prev)\n",
    "            \n",
    "            if dphinorm < .01 and dgammadnorm < .01:\n",
    "                break\n",
    "            else:\n",
    "                print (dphinorm, dgammadnorm, np.linalg.norm(phi, \"fro\"), np.linalg.norm(gammad))\n",
    "\n",
    "        gamma[:, m] = gammad\n",
    "        beta_acc[:, ixw] += phi[:, ixw]\n",
    "\n",
    "    # M-step\n",
    "    # TODO: check for numerical stability\n",
    "    beta = (beta_acc.T / np.sum(beta_acc, axis=1)).T # normalize beta rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta * ixw.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
