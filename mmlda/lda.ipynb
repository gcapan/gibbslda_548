{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as spar\n",
    "import scipy.special as spec\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from lda import LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as SKLDA\n",
    "\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 100\n",
    "V = 1000\n",
    "X = np.random.binomial(1,.3, size=M*V).reshape(M,V)\n",
    "X = spar.csr_matrix(X, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For even a reasonable setup like 10K vocabulary, 5K documents and 20 topics, the size of the tensor indexed by\n",
    "# <document, word, topic> simply explodes to 7.5G. This is why we can't explicitly keep all of $\\phi$ in the memory.\n",
    "# Instead, we iterate over the documents one by one, and accumulate the phi parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "#cProfile.run(\"lda.fit(X)\")\n",
    "%lprun -f lda.fit lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#b, g = lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "ng = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_df=.7, min_df=20)\n",
    "ngvec = vec.fit_transform(ng.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Could not find function u'lda._doc_update'.\n",
      "AttributeError: 'LDA' object has no attribute '_doc_update'"
     ]
    }
   ],
   "source": [
    "%lprun lda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "CPU times: user 3.92 s, sys: 974 ms, total: 4.89 s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_jobs=8)\n",
    "%time b, g = lda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lookup = lambda x : [k for k, v in vec.vocabulary_.items() if v == x]\n",
    "\n",
    "def topic_summaries(b):\n",
    "    bs = b.argsort()[:,-50:]\n",
    "    for i in range(5):\n",
    "        print \"Topic\", i\n",
    "        words = []\n",
    "        for j in range(bs.shape[1]):\n",
    "            words.append(lookup(bs[i,j])[0])\n",
    "        print \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklda = SKLDA(n_topics=5, learning_method=\"batch\", verbose=True)\n",
    "sklda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_summaries(sklda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "charley roby ourselves knives uwec phrase prophets mangoe ifas blindly arrogance jayne weapon refering atheists christ fires condemned excuses geneva judaism religious existence frightened irrational conviction dies 7415 eternity pagan beaverton laws batf christians eau morality kulikauskas claire beauchaine bobbe mmalt mcovingt covington aisun3 manhattan 706 n4tmi belief 0358 30602\n",
      "Topic 1\n",
      "finals souviens uic domi champs homosexual amherst rri ns rangers nd cunixc ahl ink nubus marlins windows hello faraday monica maxtor group dos6 royals psuvm russ controller cramer 165 vlb ati md dal clayton ak296 halifax ide daker wlsmith consent card promiscuous bus keller psu lcs eisa isa kkeller dalhousie\n",
      "Topic 2\n",
      "input ext anybody outputs 301 syl motorcycles fixing hydro labs behanna panels rgb vhs fuel levine ipx ecs nada stereo fan link glide harley microsoft kth thermal vga jody bar hassle jubilee inch 350 priced formats xxmessage jlevine polytechnic fxwg nec cb360t slip sphere scrolling noticed vram quantum 80ns ltd\n",
      "Topic 3\n",
      "pocket philosophical thumb personality callison quack ucs helmets constellation brakes inning helmet rbis alomar pitcher homer chemistry therapy threw disease leg golchowy leading chem 9760 1993apr6 rbi compartment noring yeast patients dumbest hrs alicea albicans intellect alleg olchowy parr pitching eskimo mss lemon shameful chastity candida n3jxp geb cadre dsl\n",
      "Topic 4\n",
      "treaty peace hezbollah shaig territories exterminated proceeded karabakh escrow bombers sera anatolia appressian massacres dbd excepted lebanon territorial privacy extermination serdar composer homeland occupied sahak hasan sdpa 1919 ohanus kurdish arf cyprus massacre melkonian istanbul kurds urartu invasion inhabitants argic asala villages turkish ottoman armenians turks armenian turkey azeris armenia\n"
     ]
    }
   ],
   "source": [
    "topic_summaries(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.product(ngvec.shape) * 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit ngvec.indices[ngvec.indptr[14]:ngvec.indptr[14+1]]\n",
    "%timeit ngvec[14, :].nonzero()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
