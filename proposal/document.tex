\documentclass[pdf]{beamer}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}

\mode<presentation>{}

\title{Extended Topic Models with Numerical Features}

\author{G\" okhan \c Capan, Ali Caner T\" urkmen}
\begin{document}
	
\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Introduction: Topic Models}
	
	\begin{itemize}
		\item {\em Unsupervised} learning, recover \emph{latent} topics in documents
		\item Can be thought of as {\em clustering}.
		\item {\bf Key Assumption:} Topics lead to distinct word distributions. Intuitive. 
	\end{itemize}
	
\end{frame}

\begin{frame}{Latent Dirichlet Allocation}
	
	\begin{itemize}
		\item Rich, probabilistic mixed membership model due to (Blei et al., 2003)
		\item Widely adopted and extended
		\item {\bf Key Idea: } 
	\end{itemize}
	
\end{frame}

\begin{frame}{Comparison of Topic Models}
	
	\begin{itemize}
		%TODO: comparison slide from blei
		\item (Blei et al., 2003)
	\end{itemize}
	
\end{frame}

\begin{frame}{Some Examples of LDA Extensions}
	
	\begin{itemize}
		\item Gokhan lit survey
	\end{itemize}
	
\end{frame}

\begin{frame}{We will propose a simple extension to LDA for working with Count Features}
	
	\begin{itemize}
		%TODO: comparison slide from blei
		\item Jointly model both word distributions and some count features in probabilistically sound framework
		\item Can then easily extend to other distributions for numeric features
		\item Close to the state of the art
	\end{itemize}
	
\end{frame}


\begin{frame}{Data Set and Features}
	
	\begin{itemize}
		\item {\bf Data Set:} News articles sampled from Anadolu Agency website. 1337 documents (can be expanded), ~3000 tokens after adjusting for document frequency. 
		\item {\bf Features:} Complexity features such as word count, sentence count, average sentence length, comma count. (TBD)
		\item {\bf Novel Features:} Etymological counts. Count the number of words from their etymological origins. Number of Arabic, Farsi, French words, etc. Source: TR Wiktionary Database Dump.
	\end{itemize}
	
\end{frame}

\begin{frame}{Learning}
	
	\begin{itemize}
		\item {\bf Variational inference: } Derive approximate inference algorithms based on a decoupling of the original model OR Variational EM-like procedures to find parameter estimates.
		\item Gibbs sampling assuming appropriate priors (tentative, out of scope for this project)
	\end{itemize}
	
\end{frame}

\begin{frame}{Conclusion}
	
	We propose two key contributions
	\begin{itemize}
		\item Put the topic modeling problem in an extended LDA framework, with numerical features
		\item Use etymological counts for the Turkish language
	\end{itemize}
	
\end{frame}

\bibliography{zotero}{}
\bibliographystyle{apalike}
\end{document}