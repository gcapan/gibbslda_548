{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as spar\n",
    "import scipy.special as spec\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from lda import LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as SKLDA\n",
    "\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 100\n",
    "V = 1000\n",
    "X = np.random.binomial(1,.3, size=M*V).reshape(M,V)\n",
    "X = spar.csr_matrix(X, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For even a reasonable setup like 10K vocabulary, 5K documents and 20 topics, the size of the tensor indexed by\n",
    "# <document, word, topic> simply explodes to 7.5G. This is why we can't explicitly keep all of $\\phi$ in the memory.\n",
    "# Instead, we iterate over the documents one by one, and accumulate the phi parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "#cProfile.run(\"lda.fit(X)\")\n",
    "%lprun -f lda.fit lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "#b, g = lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "ng = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_df=.7, min_df=20)\n",
    "ngvec = vec.fit_transform(ng.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -f lda._doc_update lda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "b, g = lda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lookup = lambda x : [k for k, v in vec.vocabulary_.items() if v == x]\n",
    "\n",
    "def topic_summaries(b):\n",
    "    bs = b.argsort()[:,-50:]\n",
    "    for i in range(5):\n",
    "        print \"Topic\", i\n",
    "        words = []\n",
    "        for j in range(bs.shape[1]):\n",
    "            words.append(lookup(bs[i,j])[0])\n",
    "        print \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   20.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.7s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=5, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklda = SKLDA(n_topics=5, learning_method=\"batch\", verbose=True)\n",
    "sklda.fit(ngvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "good your no an ca know he has up get any do all don so one there like out as just what about would me can university or at host nntp are posting not they be was if with but this article my writes have on re com you edu\n",
      "Topic 1\n",
      "ww mx p3 ei 27 mp sp 75 sk 6e mt tm p2 air 86 mc lj m5 17 sq ml 04 4t mk m_ wt 14 mr hz mu mw uw m3 bj mq 45 mv d9 lk 9v ah sl 34 as wm 1d9 pl 145 max ax\n",
      "Topic 2\n",
      "my does software would available get so about data using how information program key space all other also what has file one do some windows there your system any but which com edu at use by will not an as if have can are or you be with this on\n",
      "Topic 3\n",
      "40 league 55 are vs first apr april 23 22 21 24 as 19 50 17 season be 18 play edu 93 13 hockey 30 by ca drive new games 14 he 1993 with 11 12 year 20 16 25 game 15 will was scsi on team at 10 00\n",
      "Topic 4\n",
      "think had when don me more them which god some has his my re edu your so will at their can about no were do one an would who all people there what if he or but by with we on was have they as this be are not you\n"
     ]
    }
   ],
   "source": [
    "topic_summaries(sklda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "writes need which about article so am some please does what get anyone like one all has know will distribution would as do use there me at thanks com an university my are not re any but be if can you nntp or host this posting have on with edu\n",
      "Topic 1\n",
      "think more know will he no get any do has an up don me would like just so there one as com all my about out what can host posting nntp or they not at university are was if with but be this have article you on writes edu re\n",
      "Topic 2\n",
      "he this up out many first these about which into way do make also or those an would will some but on our well are more time one has was because been only when have with then by not had we no all as them people they their were who\n",
      "Topic 3\n",
      "will other more me know has some my just any don your we people nntp no host who so at one all about would do an posting they there was can by with what or but as if com have on article are edu not you be this writes re\n",
      "Topic 4\n",
      "my 27 april other cmu 26 this information rutgers 25 24 or pittsburgh was cs 22 23 an university with reply not may his 30 on gmt 21 17 at 19 as who 00 11 20 12 18 13 16 93 new 14 be edu by 15 10 apr 1993\n"
     ]
    }
   ],
   "source": [
    "topic_summaries(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.product(ngvec.shape) * 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit ngvec.indices[ngvec.indptr[14]:ngvec.indptr[14+1]]\n",
    "%timeit ngvec[14, :].nonzero()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
